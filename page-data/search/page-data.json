{"componentChunkName":"component---src-pages-search-tsx","path":"/search/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"rawMarkdownBody":"\n### 프로젝트: 주가 데이터에 대한 가설을 설정하고 주식 데이터 분석을 수행하기 (원티드_에듀_입학_포트폴리오)\n### 분석목표: 데이터 EDA, 시각화\n### 분석 환경: Python, Numpy, Pandas, Matplotlib\n1. numpy==1.19.5 <br>\n2. pandas==1.1.5 <br>\n3. matplotlib==3.2.2 <br>\n4. plotly==4.14.3\n\n\n### 담당업무: \n1. EDA (Exploratory Data Analysis) 탐색적 데이터 분석\n2. 프로젝트 기획\n3. 시각화\n\n\n**구글 코랩에서 시현 부탁드립니다**\n\n\n```python\n !pip install plotly==4.14.3 # --upgra\n\n```\n\n    Requirement already satisfied: plotly==4.14.3 in /opt/anaconda3/lib/python3.7/site-packages (4.14.3)\n    Requirement already satisfied: retrying>=1.3.3 in /opt/anaconda3/lib/python3.7/site-packages (from plotly==4.14.3) (1.3.3)\n    Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from plotly==4.14.3) (1.14.0)\n\n\n\n```python\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.express as px\nimport plotly\n```\n\n\n```python\nplotly.__version__ #plotly 구버전은 wide 포맷 데이터 지원을 안함. 최신버전으로 upgrade하기전 버전은 4.4.1 구버전이므로 wide 포맷을 long 포맷으로 바꿔 전처리 후 시각화 함. \n```\n\n\n\n\n    '4.14.3'\n\n\n\n\n```python\n\ndf_1= pd.read_csv('stock.adj_close.csv')\n```\n\n\n```python\ndf_1.info()\n```\n\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 2274 entries, 0 to 2273\n    Columns: 569 entries, Symbol to 2020-09-18\n    dtypes: float64(568), object(1)\n    memory usage: 9.9+ MB\n\n\n\n```python\ndf_1.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Symbol</th>\n      <th>2018-06-01</th>\n      <th>2018-06-04</th>\n      <th>2018-06-05</th>\n      <th>2018-06-07</th>\n      <th>2018-06-08</th>\n      <th>2018-06-11</th>\n      <th>2018-06-12</th>\n      <th>2018-06-14</th>\n      <th>2018-06-15</th>\n      <th>...</th>\n      <th>2020-09-07</th>\n      <th>2020-09-08</th>\n      <th>2020-09-09</th>\n      <th>2020-09-10</th>\n      <th>2020-09-11</th>\n      <th>2020-09-14</th>\n      <th>2020-09-15</th>\n      <th>2020-09-16</th>\n      <th>2020-09-17</th>\n      <th>2020-09-18</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A000020</td>\n      <td>11550.0</td>\n      <td>11750.0</td>\n      <td>11700.0</td>\n      <td>11650.0</td>\n      <td>11500.0</td>\n      <td>11500.0</td>\n      <td>11650.0</td>\n      <td>12000.0</td>\n      <td>11800.0</td>\n      <td>...</td>\n      <td>25550.0</td>\n      <td>23650.0</td>\n      <td>24550.0</td>\n      <td>25950.0</td>\n      <td>26750.0</td>\n      <td>26300.0</td>\n      <td>25400.0</td>\n      <td>25050.0</td>\n      <td>25300.0</td>\n      <td>25050.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A000030</td>\n      <td>15500.0</td>\n      <td>15950.0</td>\n      <td>16050.0</td>\n      <td>16500.0</td>\n      <td>16600.0</td>\n      <td>16650.0</td>\n      <td>16850.0</td>\n      <td>16500.0</td>\n      <td>16100.0</td>\n      <td>...</td>\n      <td>14800.0</td>\n      <td>14800.0</td>\n      <td>14800.0</td>\n      <td>14800.0</td>\n      <td>14800.0</td>\n      <td>14800.0</td>\n      <td>14800.0</td>\n      <td>14800.0</td>\n      <td>14800.0</td>\n      <td>14800.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A000040</td>\n      <td>2992.0</td>\n      <td>3021.0</td>\n      <td>3025.0</td>\n      <td>3069.0</td>\n      <td>3045.0</td>\n      <td>2984.0</td>\n      <td>2976.0</td>\n      <td>3033.0</td>\n      <td>3033.0</td>\n      <td>...</td>\n      <td>980.0</td>\n      <td>932.0</td>\n      <td>741.0</td>\n      <td>875.0</td>\n      <td>840.0</td>\n      <td>860.0</td>\n      <td>833.0</td>\n      <td>817.0</td>\n      <td>812.0</td>\n      <td>796.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A000050</td>\n      <td>13200.0</td>\n      <td>13550.0</td>\n      <td>13600.0</td>\n      <td>13800.0</td>\n      <td>13800.0</td>\n      <td>13800.0</td>\n      <td>13650.0</td>\n      <td>13850.0</td>\n      <td>14500.0</td>\n      <td>...</td>\n      <td>11100.0</td>\n      <td>11350.0</td>\n      <td>11550.0</td>\n      <td>11500.0</td>\n      <td>11550.0</td>\n      <td>11300.0</td>\n      <td>11300.0</td>\n      <td>11050.0</td>\n      <td>10900.0</td>\n      <td>10750.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A000060</td>\n      <td>20050.0</td>\n      <td>20050.0</td>\n      <td>20150.0</td>\n      <td>20050.0</td>\n      <td>20400.0</td>\n      <td>20150.0</td>\n      <td>20700.0</td>\n      <td>20950.0</td>\n      <td>20150.0</td>\n      <td>...</td>\n      <td>12700.0</td>\n      <td>12700.0</td>\n      <td>12650.0</td>\n      <td>12700.0</td>\n      <td>12800.0</td>\n      <td>13000.0</td>\n      <td>13200.0</td>\n      <td>13150.0</td>\n      <td>13150.0</td>\n      <td>13200.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 569 columns</p>\n</div>\n\n\n\n# Analyzed Facts\n1. df_1.head()로 데이터를 확인하니 symbol열은 \"주가 코드\"이고 나머지 열은 object타입의 날짜를 의미함.\n2. 데이터 타입은 float64.\n3. 행 569개, 열 2273개.\n4. 2018년 6월 1일부터 2020년 09월 18일까지의 데이터\n\n\n\n# Inquring EDA Lists (1)\n1. KOSPI와 KOSDAQ이 함께 섞여있는 데이값인가? \n2. 데이터 값이 하한가? 상한가? 종가인가? <br>\n\n\n### 1. KOSPI와 KOSDAQ이 섞여 있는 값이지 알아보기 위해선 KOSPI와 KOSDAQ의 대장주인 셀트리온 헬스케어를 조회하면 된다. <br>\n삼성전자 종목코드: 005930 <br>\n셀트리온 헬스케어: 091990\n\n\n```python\ndf = df_1.transpose()\n```\n\n\n```python\ndf.rename(columns=df.iloc[0],inplace=True)\n```\n\n\n```python\ndf = df.drop(df.index[0])\n```\n\n\n```python\ndf.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A000020</th>\n      <th>A000030</th>\n      <th>A000040</th>\n      <th>A000050</th>\n      <th>A000060</th>\n      <th>A000070</th>\n      <th>A000080</th>\n      <th>A000100</th>\n      <th>A000120</th>\n      <th>A000140</th>\n      <th>...</th>\n      <th>A900310</th>\n      <th>A900340</th>\n      <th>A950110</th>\n      <th>A950130</th>\n      <th>A950140</th>\n      <th>A950160</th>\n      <th>A950170</th>\n      <th>A950180</th>\n      <th>A950190</th>\n      <th>A950200</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-06-01</th>\n      <td>11550</td>\n      <td>15500</td>\n      <td>2992</td>\n      <td>13200</td>\n      <td>20050</td>\n      <td>120000</td>\n      <td>20200</td>\n      <td>44979</td>\n      <td>148000</td>\n      <td>9360</td>\n      <td>...</td>\n      <td>3245</td>\n      <td>NaN</td>\n      <td>7690</td>\n      <td>4970</td>\n      <td>6530</td>\n      <td>38600</td>\n      <td>13850</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2018-06-04</th>\n      <td>11750</td>\n      <td>15950</td>\n      <td>3021</td>\n      <td>13550</td>\n      <td>20050</td>\n      <td>123000</td>\n      <td>20400</td>\n      <td>42689</td>\n      <td>153000</td>\n      <td>9440</td>\n      <td>...</td>\n      <td>3245</td>\n      <td>NaN</td>\n      <td>7700</td>\n      <td>4900</td>\n      <td>6870</td>\n      <td>36400</td>\n      <td>14250</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2018-06-05</th>\n      <td>11700</td>\n      <td>16050</td>\n      <td>3025</td>\n      <td>13600</td>\n      <td>20150</td>\n      <td>123500</td>\n      <td>20350</td>\n      <td>41406</td>\n      <td>152500</td>\n      <td>9390</td>\n      <td>...</td>\n      <td>3240</td>\n      <td>NaN</td>\n      <td>7690</td>\n      <td>4940</td>\n      <td>6690</td>\n      <td>36350</td>\n      <td>14450</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2018-06-07</th>\n      <td>11650</td>\n      <td>16500</td>\n      <td>3069</td>\n      <td>13800</td>\n      <td>20050</td>\n      <td>124500</td>\n      <td>21000</td>\n      <td>41498</td>\n      <td>154000</td>\n      <td>9490</td>\n      <td>...</td>\n      <td>3245</td>\n      <td>NaN</td>\n      <td>7680</td>\n      <td>5140</td>\n      <td>6950</td>\n      <td>38700</td>\n      <td>14300</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2018-06-08</th>\n      <td>11500</td>\n      <td>16600</td>\n      <td>3045</td>\n      <td>13800</td>\n      <td>20400</td>\n      <td>123500</td>\n      <td>20750</td>\n      <td>41956</td>\n      <td>154000</td>\n      <td>9510</td>\n      <td>...</td>\n      <td>3180</td>\n      <td>NaN</td>\n      <td>7680</td>\n      <td>5280</td>\n      <td>6740</td>\n      <td>40200</td>\n      <td>14150</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 2274 columns</p>\n</div>\n\n\n\n\n```python\ndf[['A005930', 'A091990']]\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A005930</th>\n      <th>A091990</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-06-01</th>\n      <td>51300</td>\n      <td>94992</td>\n    </tr>\n    <tr>\n      <th>2018-06-04</th>\n      <td>51100</td>\n      <td>91626</td>\n    </tr>\n    <tr>\n      <th>2018-06-05</th>\n      <td>51300</td>\n      <td>91159</td>\n    </tr>\n    <tr>\n      <th>2018-06-07</th>\n      <td>50600</td>\n      <td>93122</td>\n    </tr>\n    <tr>\n      <th>2018-06-08</th>\n      <td>49650</td>\n      <td>92655</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2020-09-14</th>\n      <td>60400</td>\n      <td>99200</td>\n    </tr>\n    <tr>\n      <th>2020-09-15</th>\n      <td>61000</td>\n      <td>99700</td>\n    </tr>\n    <tr>\n      <th>2020-09-16</th>\n      <td>61000</td>\n      <td>99700</td>\n    </tr>\n    <tr>\n      <th>2020-09-17</th>\n      <td>59500</td>\n      <td>98100</td>\n    </tr>\n    <tr>\n      <th>2020-09-18</th>\n      <td>59300</td>\n      <td>98400</td>\n    </tr>\n  </tbody>\n</table>\n<p>568 rows × 2 columns</p>\n</div>\n\n\n\n\n```python\n# df[['A314130']] #이 코드를 돌리면 에러가 남. 즉, KONEX 주가 1위인 \"지놈앤컴퍼니\"는 찾을 수 없음. \n```\n\n**결과:** 데이터는 KOSPI와 KOSDAQ이 섞여있는 주가 데이터임을 알 수 있다. 혹시 몰라 KONEX 주가 또한 포함한지 KONEX의 1위 종목인 \"지놈앤컴퍼니\" 종목 코드를 탐색해봤지만 결과가 안나와 KONEX는 주가는 포함하지 않은 데이터라는 것을 알 수 있었다.\n\n### 2. 주어진 데이터가 하한가인지, 상한가인지, 종가인지 알 수 있는 방법은 네이버 증권에 나와있는 삼성전자의 하한가, 상한가 종가를 데이터의 특정 날의 삼성전자 데이터와 대조하면 알 수 있다. \n\n\n\n```python\n#pip install finance-datareader\n```\n\n\n```python\nimport FinanceDataReader as fdr\n```\n\n\n```python\nsamsung_2020_09_18 = fdr.DataReader('005930','2020-09-18','2020-09-18')\n```\n\n\n```python\nsamsung_2020_09_18\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>Change</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-09-18</th>\n      <td>59800</td>\n      <td>59900</td>\n      <td>59100</td>\n      <td>59300</td>\n      <td>18884571</td>\n      <td>-0.003361</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n**결과:** 주어진 데이터에서 삼성전자의 2020년 9월 18일 값은 크롤링 해서 가져온 삼성전자의 일일 주가 데이터에서 종가와 같다. 즉, 데이터들의 값은 종가 가격이다.\n\n\n# Analyzed Facts\n1. 데이터는 KOSPI와 KOSDAQ이 섞여있는 주가 데이터임을 알 수 있다.\n2. 데이터의 값은 일일 종가 데이터임을 확인했다.\n\n# 가설: \n주어진 데이터의 종가 가격은 2020년 3월 17일에서 19일 코스피와 코스닥은 큰 폭으로 하락을 했을것이다.<br>\n이유:<br> \n\"3월 17일, 연기금이 코스피200 선물지수하락에 2배로 배팅하는 KODEX200 선물인버스2X ETF를 매수해 주식투자자들에게 커다란 충격을 안겨주었다. 연기금이 헷지 차원에서 KODEX 200선물인버스2X ETF를 매수하는 일이 간간이 있기는 했지만 이날 코스피 종가는 1672포인트였다. 아무리 헷지용으로 매수했다 해도 그동안 한국 주가지수가 무너지면 매수로 방어해주던 연기금이 코스피 1672포인트에서 헷지용으로 KODEX 200선물인버스2X를 매수했다는 것은 앞으로 더 크게 폭락할 거라 본 것을 의미했다. 실제로 바로 다음날인 3월 18일에 코스피는 1591포인트로 마감했고, 그 다음날인 3월 19일에는 장중 2020년 주가 대폭락 최저점인 1439.43포인트를 기록했다.\"\n\n[출처](https://https://namu.wiki/w/2020%EB%85%84%20%EC%A3%BC%EA%B0%80%20%EB%8C%80%ED%8F%AD%EB%9D%BD)\n\n\n#Inquring EDA Lists (2)\n1. 동적 시각화를 할 수 있는 plotly를 사용하기 위해 long 포맷으로 바꾸고 시각화 하기.\n2. 만약 그래프가 복잡하다면 KOSPI와 KOSDAQ 종가를 나눠 시각화 하기.\n\n### 1. \"데이터를 시각화 하면 2020년 3월 19일에 종가 가격은 하락 했을것이다\"라는 가설을 입증하기 위한 데이터 탐색 후 시각화\n\n\n```python\nreset_df = df.reset_index()\n```\n\n\n```python\nreset_df.info()\n```\n\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 568 entries, 0 to 567\n    Columns: 2275 entries, index to A950200\n    dtypes: object(2275)\n    memory usage: 9.9+ MB\n\n\n\n```python\nreset_df.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>A000020</th>\n      <th>A000030</th>\n      <th>A000040</th>\n      <th>A000050</th>\n      <th>A000060</th>\n      <th>A000070</th>\n      <th>A000080</th>\n      <th>A000100</th>\n      <th>A000120</th>\n      <th>...</th>\n      <th>A900310</th>\n      <th>A900340</th>\n      <th>A950110</th>\n      <th>A950130</th>\n      <th>A950140</th>\n      <th>A950160</th>\n      <th>A950170</th>\n      <th>A950180</th>\n      <th>A950190</th>\n      <th>A950200</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-06-01</td>\n      <td>11550</td>\n      <td>15500</td>\n      <td>2992</td>\n      <td>13200</td>\n      <td>20050</td>\n      <td>120000</td>\n      <td>20200</td>\n      <td>44979</td>\n      <td>148000</td>\n      <td>...</td>\n      <td>3245</td>\n      <td>NaN</td>\n      <td>7690</td>\n      <td>4970</td>\n      <td>6530</td>\n      <td>38600</td>\n      <td>13850</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-06-04</td>\n      <td>11750</td>\n      <td>15950</td>\n      <td>3021</td>\n      <td>13550</td>\n      <td>20050</td>\n      <td>123000</td>\n      <td>20400</td>\n      <td>42689</td>\n      <td>153000</td>\n      <td>...</td>\n      <td>3245</td>\n      <td>NaN</td>\n      <td>7700</td>\n      <td>4900</td>\n      <td>6870</td>\n      <td>36400</td>\n      <td>14250</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2018-06-05</td>\n      <td>11700</td>\n      <td>16050</td>\n      <td>3025</td>\n      <td>13600</td>\n      <td>20150</td>\n      <td>123500</td>\n      <td>20350</td>\n      <td>41406</td>\n      <td>152500</td>\n      <td>...</td>\n      <td>3240</td>\n      <td>NaN</td>\n      <td>7690</td>\n      <td>4940</td>\n      <td>6690</td>\n      <td>36350</td>\n      <td>14450</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-06-07</td>\n      <td>11650</td>\n      <td>16500</td>\n      <td>3069</td>\n      <td>13800</td>\n      <td>20050</td>\n      <td>124500</td>\n      <td>21000</td>\n      <td>41498</td>\n      <td>154000</td>\n      <td>...</td>\n      <td>3245</td>\n      <td>NaN</td>\n      <td>7680</td>\n      <td>5140</td>\n      <td>6950</td>\n      <td>38700</td>\n      <td>14300</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-06-08</td>\n      <td>11500</td>\n      <td>16600</td>\n      <td>3045</td>\n      <td>13800</td>\n      <td>20400</td>\n      <td>123500</td>\n      <td>20750</td>\n      <td>41956</td>\n      <td>154000</td>\n      <td>...</td>\n      <td>3180</td>\n      <td>NaN</td>\n      <td>7680</td>\n      <td>5280</td>\n      <td>6740</td>\n      <td>40200</td>\n      <td>14150</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 2275 columns</p>\n</div>\n\n\n\n\n```python\nlong_format= reset_df.melt(id_vars='index')\n```\n\n\n```python\nlong_format.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>variable</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-06-01</td>\n      <td>A000020</td>\n      <td>11550</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-06-04</td>\n      <td>A000020</td>\n      <td>11750</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2018-06-05</td>\n      <td>A000020</td>\n      <td>11700</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-06-07</td>\n      <td>A000020</td>\n      <td>11650</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-06-08</td>\n      <td>A000020</td>\n      <td>11500</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\nlong_format.info() # 월단위, 주단위 로 ??\n```\n\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 1291632 entries, 0 to 1291631\n    Data columns (total 3 columns):\n     #   Column    Non-Null Count    Dtype \n    ---  ------    --------------    ----- \n     0   index     1291632 non-null  object\n     1   variable  1291632 non-null  object\n     2   value     1222282 non-null  object\n    dtypes: object(3)\n    memory usage: 29.6+ MB\n\n\n\n```python\nfig = px.line(long_format, #약 5분 소요됨.\n              x='index', \n              y='value',\n              color='variable'\n              )\n```\n\n\n```python\nfig\n```\n\n결과: 3월 19일날 KOSPI와 KOSDAQ의 종가 가격이 떨어진 것을 볼 수 있다. 가장 많은 타격을 입은 회사는 종목코드가 A003240인 태광산업이다. \n\n\n### 2. 만약 그래프가 복잡하다면 KOSPI와 KOSDAQ 종가를 나눠 시각화 하기위한 데이터 탐색, 전처리 후 시각화\n\n#### 2-1.KOSDAQ 시각화\n\n\n\n```python\n# 코스닥\ndf_KOSDAQ = fdr.StockListing('KOSDAQ')\ndf_KOSDAQ.head()\n```\n\n\n```python\ndf_KOSDAQ.info\n```\n\n\n```python\ndf_KOSDAQ['Symbol']\n```\n\n\n```python\ndf_KOSDAQ_symbol = df_KOSDAQ['Symbol']\n```\n\n\n```python\ndf_KOSDAQ_symbol = 'A' + df_KOSDAQ_symbol\n```\n\n\n```python\ndf_KOSDAQ_symbol_listed = list(df_KOSDAQ_symbol)\n```\n\n\n```python\n# print(df_KOSDAQ_symbol_listed)\n```\n\n\n```python\nKOSDAQ_sorted_from_data = long_format.query(\"variable == @df_KOSDAQ_symbol_listed\")\n```\n\n\n```python\nKOSDAQ_sorted_from_data.head()\n```\n\n\n```python\nKOSDAQ_sorted_from_data_fig = px.line(KOSDAQ_sorted_from_data, x='index', y='value',color='variable')\n```\n\n\n```python\nKOSDAQ_sorted_from_data_fig\n```\n\n#### 2-2.KOSPI 시각화\n\n\n```python\n# 코스피\ndf_KOSPI = fdr.StockListing('KOSPI')\ndf_KOSPI.head()\n```\n\n\n```python\ndf_KOSPI.info\n```\n\n\n```python\ndf_KOSPI['Symbol']\n```\n\n\n```python\ndf_KOSPI_symbol = df_KOSPI['Symbol']\n```\n\n\n```python\ndf_KOSPI_symbol = 'A' + df_KOSPI_symbol\n```\n\n\n```python\ndf_KOSPI_symbol_listed = list(df_KOSPI_symbol)\n```\n\n\n```python\n#print(df_KOSPI_symbol_listed)\n```\n\n\n```python\nKOSPI_sorted_from_data = long_format.query(\"variable == @df_KOSPI_symbol_listed\")\n```\n\n\n```python\nKOSPI_sorted_from_data.head()\n```\n\n\n```python\nKOSPI_sorted_from_data_fig = px.line(KOSPI_sorted_from_data, x='index', y='value',color='variable')\n```\n\n\n```python\nKOSPI_sorted_from_data_fig\n```\n\n# Analyzed Facts\n1. KOSPI, KOSDAQ 각각 2020년 3월에 큰 폭으로 종가가 하락한것을 그래프로 볼 수 있다.\n\n# 가설:\nKOSPI는 대기업이 상장되있고 KOSDAQ은 중견 벤처기업이 상장되있는만큼, 위기일 수록 KOSDAQ에 상장된 회사의 성장률은 KOSPI에 상장된 회사 위태로울 것이다.\n\n\n# Inquring EDA Lists (3)\n1. KOSDAQ 일일 성장률 계산\n2. KOSPI 일일 성장률 계산 \n3. KOSDAQ,KOSPI 시각화, 차트 성장률 비교\n\n### 1. KOSDAQ 일일 성장률 계산\n\n\n```python\nKOSDAQ_sorted_from_data.info()\n```\n\n\n```python\nKOSDAQ_sorted_from_data_wide_format = KOSDAQ_sorted_from_data.pivot(index='index',columns='variable', values='value')\n```\n\n\n```python\nKOSDAQ_sorted_from_data_wide_format\n```\n\n\n```python\nKOSDAQ_growth_ratio = KOSDAQ_sorted_from_data_wide_format / KOSDAQ_sorted_from_data_wide_format.iloc[0] -1\n```\n\n\n```python\nKOSDAQ_growth_ratio.columns\n```\n\n\n```python\nKOSDAQ_growth_ratio\n```\n\n\n```python\nKOSDAQ_growth_ratio_mean = KOSDAQ_growth_ratio.mean(axis=1)\n```\n\n\n```python\nKOSDAQ_growth_ratio_mean\n```\n\n\n```python\nKOSDAQ_growth_ratio_mean = pd.DataFrame(KOSDAQ_growth_ratio.mean(axis=1))\n```\n\n\n```python\nKOSDAQ_growth_ratio_mean.rename(columns=KOSDAQ_growth_ratio_mean.iloc[0],inplace=True)\n```\n\n\n```python\nKOSDAQ_growth_ratio_mean.columns=['KOSDAQ_average_growth_ratio']\n```\n\n\n```python\nKOSDAQ_growth_ratio_mean\n```\n\n\n```python\nreset_KOSDAQ_growth_ratio_mean = KOSDAQ_growth_ratio_mean.reset_index()\n```\n\n\n```python\nreset_KOSDAQ_growth_ratio_mean\n```\n\n\n```python\nKOSDAQ_growth_ratio_mean_long_format = reset_KOSDAQ_growth_ratio_mean.melt(id_vars='index')\n```\n\n\n```python\nKOSDAQ_growth_ratio_mean_long_format\n```\n\n\n```python\nKOSDAQ_growth_ratio_mean_long_format_fig = px.line(KOSDAQ_growth_ratio_mean_long_format, x='index', y='value',color='variable')\n```\n\n\n```python\nKOSDAQ_growth_ratio_mean_long_format_fig\n```\n\n### 2. KOSPI 일일 성장률 계산\n\n\n```python\nKOSPI_sorted_from_data_wide_format = KOSPI_sorted_from_data.pivot(index='index',columns='variable', values='value')\n```\n\n\n```python\nKOSPI_growth_ratio = KOSPI_sorted_from_data_wide_format / KOSPI_sorted_from_data_wide_format.iloc[0] -1\n```\n\n\n```python\nKOSPI_growth_ratio_mean = KOSPI_growth_ratio.mean(axis=1)\n```\n\n\n```python\nKOSPI_growth_ratio_mean = pd.DataFrame(KOSPI_growth_ratio.mean(axis=1))\n```\n\n\n```python\nKOSPI_growth_ratio_mean.columns=['KOSPI_average_growth_ratio']\n```\n\n\n```python\nreset_KOSPI_growth_ratio_mean = KOSPI_growth_ratio_mean.reset_index()\n```\n\n\n```python\nKOSPI_growth_ratio_mean_long_format = reset_KOSPI_growth_ratio_mean.melt(id_vars='index')\n```\n\n\n```python\nKOSPI_growth_ratio_mean_long_format_fig = px.line(KOSPI_growth_ratio_mean_long_format, x='index', y='value',color='variable')\n```\n\n\n```python\nKOSPI_growth_ratio_mean_long_format_fig\n```\n\n### 3. KOSDAQ,KOSPI 시각화, 차트 성장률 비교\n\n\n```python\nKOSDAQ_KOSPI_mean_table_merged = pd.merge(KOSDAQ_growth_ratio_mean,KOSPI_growth_ratio_mean,on='index')\n```\n\n\n```python\nKOSDAQ_KOSPI_mean_table_merged = KOSDAQ_KOSPI_mean_table_merged.reset_index()\n```\n\n\n```python\nKOSDAQ_KOSPI_mean_table_merged_long_format = KOSDAQ_KOSPI_mean_table_merged.melt(id_vars='index')\n```\n\n\n```python\nKOSDAQ_KOSPI_mean_table_merged_long_format_fig = px.line(KOSDAQ_KOSPI_mean_table_merged_long_format, x='index', y='value',color='variable')\n```\n\n\n```python\nKOSDAQ_KOSPI_mean_table_merged_long_format_fig\n```\n\n\n```python\nKOSDAQ_KOSPI_average_growth_rate = pd.DataFrame(KOSDAQ_KOSPI_mean_table_merged.mean()) #데이터 프레임을 씌웠을때\n```\n\n\n```python\nKOSDAQ_KOSPI_average_growth_rate.columns=['growth_rate']\n```\n\n\n```python\nKOSDAQ_KOSPI_average_growth_rate\n```\n\n\n```python\nKOSDAQ_KOSPI_average_growth_rate.plot(kind ='bar',figsize=(8,4), rot=0)\n\n```\n\n# Analyzed Facts\n1. 지난 2년동안 KOSPI와 KOSDAQ 성장률은 마이너스인것으로 탐색되었다.\n2. 더 나아가서 우리가 세운 가설 \"KOSPI는 대기업이 상장되있고 KOSDAQ은 중견 벤처기업이 상장되있는만큼, 위기일 수록 KOSDAQ에 상장된 회사의 성장률은 KOSPI에 상장된 회사 위태로울 것이다.\"과 반대되는 결과가 탐색되었다. 지난 2년간 KOSPI의 전체적인 성장률이 KOSDAQ의 성장률보다 마이너스인것으로 확인된다.\n","excerpt":"프로젝트: 주가 데이터에 대한 가설을 설정하고 주식 데이터 분석을 수행하기 (원티드에듀입학_포트폴리오) 분석목표: 데이터 EDA, 시각화 분석 환경: Python, Numpy, Pandas, Matplotlib numpy==1.19.5  pandas…","fields":{"slug":"/test/"},"frontmatter":{"date":"Apr 22, 2021","title":"test","tags":["데이터 분석 프로젝트","데이터 분석 입사 프로젝트","R studio 탐색적 분석하기","서울생활인구 분석"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n\n\n- 프로젝트: Cohort, Retention, RFM 을 통해 Retail 데이터 분석<br>\n- 분석 목표:퍼널 분석을 통해 유입된 고객이 있다고 가정을 하고,Cohort 분석을 통해 우리가 데려온 고객들을 얼마나 잘 자키고 있는지 retention을 측정을 하고, 고객별로 적절한 마케팅 메세지를 보내주기 위해 RFM을 통해서 고객들을 세그먼트한다.<br>\n- 분석 환경: Google SQL(추출), EXCEL(시각화)<br>\n- 담당 업무: <br>1. EDA 탐색점 데이터 분석<br>2.프로젝트 기획<br>3.퍼널 분석 후 시각화<br>4.Retention 계산<br> 5.RFM 계산 후 시각화<br>\n- 데이터 출처: https://www.kaggle.com/jihyeseo/online-retail-data-set-from-uci-ml-repo\n\n\n<br>\n\n# 코호트 분석 <br>\n### Step 1. 코호트의 기간 기준 정하기  \n\n```sql\nSELECT DISTINCT (DATE(invoice_date) ) AS invoice_date\nFROM `axial-coyote-310511.cohort.cohort_project`\nORDER BY invoice_date\nLIMIT 300\n```\n<p align=\"center\"><img src=\"./2.png\"  height=\"200px\" width=\"100px\"></p> \n우선 코호트 기간을 한 달로 잡았다. 이유는 Retail 산업을 고려하면 코호트 기간을 daily로 잡는 것이 맞겠지만, 데이터를 살펴보니 daily 로그가 많이 빠져있었기 때문이다.\n\n### Step2. 코호트 index 구하기.\n1. 처음 구매한  시점부터 고객들을 월별로 나눠서  트랙킹 하기.\n2. 고객별로 고객이 가장 처음에 구매한 일자 구하기.\n```sql\nSELECT customer_id, DATE(MIN(invoice_date)) AS cohort_day\nFROM cohort.cohort_project\nGROUP BY customer_id\n```\n3. 원본 데이터에 고객별로 처음 구매한 날자 테이블을 JOIN해주기\n4. 우리가 궁극적으로 구해야 하는 것은 코호트 Index. \n코호트 Index란, 고객이 첫 구매하고 재구매가 일어났는지알 알기 위해서 재구매 날과 첫 구매 날의 차를 계산한 결과.\n\n```sql\nWITH first_purchase AS (\nSELECT customer_id, DATE(MIN(invoice_date)) AS cohort_day\nFROM cohort.cohort_project\nGROUP BY customer_id\n)\nSELECT c.*, f.cohort_day, DATE_DIFF(DATE(invoice_date), cohort_day, MONTH) AS cohort_index\nFROM cohort.cohort_project c LEFT JOIN first_purchase f\nON c.customer_id = f.customer_id\n```\n### Step3. 코호트 그룹 만들기. \n같은 달에 첫 구매한 애들끼리 그룹으로 묶어주기.\n1. 코호트 기간을 월로 정했으나 cohort_day를 보면 일까지 나와있다. 코호트로 그룹을 만들기 위해서 월로 바꿔줘야 한다. 함수 DATE_TRUNC()이용.\n\n```sql\nWITH first_purchase AS (\nSELECT customer_id, DATE(MIN(invoice_date)) AS cohort_day\nFROM cohort.cohort_project\nGROUP BY customer_id\n)\nSELECT c.*, f.cohort_day, DATE_DIFF(DATE(invoice_date), cohort_day, MONTH) AS cohort_index, DATE_TRUNC(cohort_day, MONTH) AS cohort_group\nFROM cohort.cohort_project c LEFT JOIN first_purchase f\nON c.customer_id = f.customer_id\n \n```\n<br>\n\n<p align=\"center\"><img src=\"./3.png\"  height=\"130px\" width=\"130px\"></p> \n4월 7일을 4월 1일로 3월 23일은 3월1로 바뀐것을 볼 수 있다. \n\n2. 코호트 그룹별로, 코호트 인덱스별로 해당하는 고객수가 얼마나 되는지 확인하기. \n\n```sql\nWITH first_purchase AS (\nSELECT customer_id, DATE(MIN(invoice_date)) AS cohort_day\nFROM cohort.cohort_project\nGROUP BY customer_id\n)\nSELECT  cohort_group, cohort_index, COUNT(DISTINCT customer_id) AS customer_count\nFROM (\n   SELECT c.*, f.cohort_day, DATE_DIFF(DATE(invoice_date), cohort_day, MONTH) AS cohort_index, DATE_TRUNC(cohort_day, MONTH) AS cohort_group\n   FROM cohort.cohort_project c LEFT JOIN first_purchase f\n   ON c.customer_id = f.customer_id)\nGROUP BY cohort_group, cohort_index\n\n```\n우리가 retention을 볼 때 다음 달에 한번 구매하던 두 번 구매하던 상관없이 이달에 첫 구매한 100명 중에 몇 명이 시간이 지나도 살아있나를 보기 위해서 customer_id를 셀 때 한 사람이 여러분 구매할 수 있으니 DISTINCT을 써줘야 한다.\n\n![png](./4.png)\n\n표를 가로로 보면 유저의 라이프 사이클을 볼 수 있고 세로로 보면 프로덕트의 라이프 사이클을 볼 수 있다.\n가로로 보면 2010년 12월 1일에 가입한 사람들은 885명인데 1달이 지났을 때 재구매 비율이 37%, 그다음 달부터는 안정적으로 유지가 되는 것을 볼 수 있다.\n두 번째 그룹을 보더라도 처음에 22%로 떨어지더라도 그다음부터는 매우 안정적으로 재구매가 일어난다.\n\n값을 세로로 본다면 (가장 최근 데이터는 염두 해주지 말자. 가장 최근 데이터가 2011년 12월 중순이기 때문에 12달 데이터가 완벽히 수집이 안된 상태다.) 데이터가 크게 안 좋아지지는 않는다.\n\n유저의 리텐션은 2달 후부터는 매우 안정적이고, 그리고 프로덕트 라이프 사이클을 봐도 건강한 편이다.\n특이한 점은 2010년 12월에 데려온 고객들이 높은 retention을 보여주는 것. 첫 코호트 고객들에게 회사가 특별한 이벤트를 진행했을 가능성이 있다고 본다. <BR>\n엑셀 파일: <a href=\"./sql_cohort_result.csv\" download>sql_cohort_csv</a>\n\n# 리텐션\n코호트 분석과 클레식 리텐션, 레인지 리텐션을 구하는 방법이 같기 때문에 롤링 리텐션으로 데이터를 분석해보려고 한다. \n\n```sql\nSELECT COUNT(customer_id) as total_customer\n   , COUNT(CASE WHEN  diff_day >=29 THEN 1 END) AS retention_customer\n   , COUNT(CASE WHEN  diff_day >=29 THEN 1 END)/COUNT(customer_id) AS rolling_retention_30\nFROM (\nSELECT customer_id, DATE(MIN(invoice_date)) AS first_purchase,\n   DATE(MAX(invoice_date)) AS recent_purchse,\n   DATE_DIFF(DATE(MAX(invoice_date)),DATE(MIN(invoice_date)),DAY) AS diff_day\nFROM `axial-coyote-310511.cohort.cohort_project`\ngroup by customer_id)\n\n```\n<p align=\"left\"><img src=\"./5.png\"  height=\"200px\" width=\"550px\"></p> \n결과: 데이터상 30일이 지나도 여전히 살아있는 고객은 60%정도이다.\n\n# RFM\n\n```sql\nSELECT customer_id,\nMAX(invoice_date) AS recent_purchase,\nDATE_DIFF(DATE('2011-12-10'),DATE(MAX(invoice_date)), DAY) AS recency,\nCOUNT(invoice_no) AS frequency,\nSUM(quantity * unit_price) AS monetary\n \nFROM `axial-coyote-310511.cohort.cohort_project`\nGROUP BY customer_id\n```\n<a href=\"./RFM.csv\" download>RFM_분석_결과_csv</a>","excerpt":"프로젝트: Cohort, Retention, RFM 을 통해 Retail 데이터 분석 분석 목표:퍼널 분석을 통해 유입된 고객이 있다고 가정을 하고,Cohort 분석을 통해 우리가 데려온 고객들을 얼마나 잘 자키고 있는지 retention을 측정을 …","fields":{"slug":"/Cohort_Project/"},"frontmatter":{"date":"Apr 13, 2021","title":"코호트, 리텐션, RFM을 이용한 데이터 분석 프로젝트","tags":["코호트분석","리텐션분석","RFM분석","데이터분석가","데이터분석프로젝트","SQL","Ggoogle SQL Query"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"# Interviews \n<img src=\"./hacker_rank_interviews.png\">\n\n```sql\nSELECT\ncontest_id, hacker_id, name,\ntotal_submissions,total_accepted_submissions,total_views,total_unique_views\nFROM\n(SELECT \ncontest_id, \nhacker_id, \nname,\nSUM(total_submissions) total_submissions,\nSUM(total_accepted_submissions) total_accepted_submissions\nFROM\nContests\nleft join\nColleges USING(contest_id)\nleft join\nChallenges USING(college_id)\nleft join\nSubmission_Stats USING(challenge_id)\nGROUP BY contest_id, hacker_id, name) A\n\ninner join\n\n(\nSELECT \ncontest_id, hacker_id, name,\nSUM(total_views ) total_views,\nSUM(total_unique_views ) total_unique_views\nFROM\nContests\nleft join\nColleges USING(contest_id)\nleft join\nChallenges USING(college_id)\nleft join\nView_Stats USING(challenge_id)\nGROUP BY contest_id, hacker_id, name) B\n\nUSING(contest_id, hacker_id, name)\n\nWHERE total_submissions IS NOT NULL AND total_accepted_submissions IS NOT NULL AND total_views IS NOT NULL AND total_unique_views IS NOT NULL\n\n```\n\n\n# Contest Leaderboard\n\nYou did such a great job helping Julia with her last coding contest challenge that she wants you to work on this one, too!\n\nThe total score of a hacker is the sum of their maximum scores for all of the challenges. \nWrite a query to print the hacker_id, name, and total score of the hackers ordered by the descending score. \nIf more than one hacker achieved the same total score, then sort the result by ascending hacker_id. Exclude all hackers with 0 a total score of from your result.\n\n번역\n너가 지난번에 줄리아 코딩대회를 잘 도와줘서 줄리아가 이번에도 너의 도움을 원해.\n해커 한명의 총 점수는 그 한명이 도전한 문제의 점수 중 가장 높게 나온 점수들의 합으로 정의한다. \n해커 아이디, 이름, 그리고 해커의 총 점수를 출력해라. \n점수를 내림차순으로 정렬해라.\n만약 한 명이상의 선수가 같은 점수를 받았다면 hacker_id열 기준 오름차순으로 정렬해라.\n총 점수가 0점인 사람은 출력에서 배제하라. \n\n### Input Format\nThe following tables contain contest data: <br>\nHackers: The hacker_id is the id of the hacker, and name is the name of the hacker. <br>\n\n\n|Column|Type|\n|:---:|:---:|\n|hacker_id|Integer|\n|name|String|\n\nSubmissions: The submission_id is the id of the submission, hacker_id is the id of the hacker who made\nthe submission, challenge_id is the id of the challenge for which the submission belongs to, and score is\nthe score of the submission.\n\n|Column|Type|\n|:---:|:---:|\n|submission_id|Integer|\n|hacker_id|Integer|\n|challenge_id|Integer|\n|score|Integer|\n\n### Sample Input\nHackers Table:\n\n|hacker_id|name|\n|:---:|:---:|\n| 4071  | name  |\n| 4806  | Angela  |\n| 26071  | Frank  |\n| 49438  | Patrick  |\n| 74842  | Lisa  |\n| 80305  | Kimberly  |\n| 84072  | Bonnie  |\n| 87868  | Michael  |\n| 92118  | Todd  |\n| 95895  | Joe  |\n\n<br>\n\nSubmissions Table: <br>\n\n|submission_id|hacker_id|challenge_id|score|\n|:---:|:---:|:---:|:---:|\n|67194|<span style=\"color:blue\">74842</span>|<span style=\"color:blue\">63132</span>|<span style=\"color:blue\">76</span>|\n|64479   |<span style=\"color:blue\">74842</span>   |<span style=\"color:blue\">19797</span>   |<span style=\"color:blue\">98</span>   |\n|40742   |26071   |49593   |20   |\n|17513   |4806   |49593   |32   |\n|69846   |80305   |19797   |19   |\n|41002   |26071   |89343   |36   |\n|52826   |49438   |49593   |9   |\n|31093   |26071   |19797   |2   |\n|81614   |84072   |49593   |100   |\n|44829   |26071   |89343   |17   |\n|75147   |80305   |49593   |48   |\n|14115   |4806   |49593   |76   |\n|6943   |<span style=\"color:red\">4071</span>   |<span style=\"color:red\">19797</span>   |<span style=\"color:red\">95</span>   |\n|12855   |4806   |25917   |13   |\n|73343   |80305   |49593   |42   |\n|84264   |84072   |63132   |0   |\n|9951   |<span style=\"color:red\">4071</span>   |<span style=\"color:red\">49593</span>   |<span style=\"color:red\">43</span>   |\n|45104   |49438   |25917   |34   |\n|53795   |<span style=\"color:blue\">74842</span>   |<span style=\"color:blue\">19797</span>   |<span style=\"color:blue\">5</span>   |\n|26363   |26071   |19797   |5   |\n|10063   |<span style=\"color:red\">4071</span>   |<span style=\"color:red\">49593</span>   |<span style=\"color:red\">96</span>   |\n\n<br>\nExplanation <br>\nHacker 4071 submitted solutions for challenges 19797 and 49593, so the total score = 95 + max(43,96) = 101\n<br>\nHacker 74842 submitted solutions for challenges 19797 and 63132, so the total score = max(98,5) + 76 = 174\n<br>\n<br>\n분석\n<br>\n해커랭크 mysql서버는 window function이 안된다. <br>\n처음에는 코드가 잘못된줄 알고 계속 들여다 봤지만 안되서 ms sql server로 바꿔서 진행했다. \n해커랭크 mysql서버가 예전버전이어서 window function이 안되는것같아 방법을 2개로 나눠서 진행했다. 첫 번째 방법은 회사 mysql 서버에서 window function이 돌아갈때를 가정하고 풀었고 두 번째 방법은 회사 mysql 서버가 예전 버전이라고 가정하고 window function 없이 풀었다. <br>\n\n방법 1: \n1. 첫 번째 테이블 만들기\n2. 첫 번째 select 절 만들기\n3. 두 번째 테이블절 지정.\n4. 두 번째 select절 조건 작성\n5. 두 번째 select절 출력\n6. 세 번째 테이블 지정\n7. 세 번째 select절 조건 만들기<br>\n7-1. hacker_id당, name당 score 점수 합산 <br>\n7-2. 점수 합산이 0인것은 제외 <br>\n7-3. 가장 높은 점수 부터 내림차순으로 정렬 출력, 같은 점수가 있다면 hacker_id 올림차순으로 정렬 출력.\n8. 세 번째 select절 출력\n\n```sql\n/*\n8. 세 번째 select절 출력\n*/\nselect hacker_id, name, sum(score) \n/*\n6. 세 번째 테이블 지정\n*/\nfrom\n(\n    /*\n    5. 두 번째 select절 출력\n    */\n    select t.hacker_id, \n           t.name, \n           t.challenge_id, \n           t.score, \n           t.rnk\n    /*\n    3. 두 번째 테이블절 지정. \n    */\n    from (\n        /*\n        2. 첫 번째 select 절 만들기\n        해커 아이디, 해커 이름, 해커가 도전했을 떄 부여받은 아이디, 점수, 그리고 서브미션 해커아이디와 서브미션 도전 아이디 기준으로 파티션을 나누고 서브미션 점수 열 기준으로 내림차순하는 하는 열을 rnk라고 이름을 만들고 출력한다. \n        */\n        select hackers.hacker_id as hacker_id, \n               hackers.name as name , \n               submissions.challenge_id as challenge_id,\n               submissions.score as score,\n               row_number() over (partition by submissions.hacker_id, submissions.challenge_id order by submissions.score desc) as rnk\n         /*\n         1. 첫 번째 테이블 만들기\n         해커 태이블과 서브미션 테이블을 해커테이블의 해커아이디와 서브미션 테이블의 해커아이디 기준으로 inner join을 해준다.\n         */        \n        from hackers inner join submissions on hackers.hacker_id = submissions.hacker_id) t\n    /*\n    4. 두 번째 select절 조건 작성\n    rnk가 1인 행만 출력. rnk가 1이라는 뜻은 같은 문제를 풀었을 때 가장 높은 점수를 받은 값을 의미함.\n    */\n    where rnk = 1\n) t2\n/*7. 세 번째 select절 조건 만들기*/\ngroup by hacker_id, name\nhaving sum(score)  != 0\norder by sum(score)  desc, hacker_id\n```\n방법2\n\n```sql\nselect hackers.hacker_id, hackers.name, sum(max_score) as total_score\n\n    from(\n    select hacker_id, challenge_id, max(score) as max_score\n    from Submissions\n    group by hacker_id, challenge_id\n    ) t inner join hackers on t.hacker_id = hackers.hacker_id \ngroup by hackers.hacker_id, hackers.name\nhaving total_score != 0 \norder by total_score DESC, hacker_id\n```\n\n# New Companies\n목표: print the company_code, founder name, total number of lead managers, total number of senior managers, total number of managers, and total number of employees.\n조건: Order your output by ascending company_code. \n주의사항: \n1. the table may contain duplicate records.\n2. the company_code is string, so the sorting should not be numeric. \n\n방법1: Inner Join 구문 이용하기\n```sql\nselect c.company_code\n, c.founder\n, count(distinct lm.lead_manager_code)\n, count(distinct sm.senior_manager_code)\n, count(distinct m.manager_code)\n, count(distinct e.employee_code)\nfrom company c \ninner join lead_manager lm on c.company_code = lm.company_code\ninner join senior_manager sm on lm.lead_manager_code = sm.lead_manager_code\ninner join manager m on sm.senior_manager_code = m.senior_manager_code\ninner join employee e on m.manager_code = e.manager_code\n\ngroup by c.company_code, c.founder\norder by c.company_code\n```\n\n방법 2: 서브쿼리 이용하기 \n```sql \nselect c.company_code, \nc.founder,\n(select count(distinct lead_manager_code) from lead_manager where c.company_code = company_code),\n(select count(distinct senior_manager_code) from senior_manager where c.company_code = company_code),\n(select count(distinct manager_code) from manager where c.company_code = company_code),\n(select count(distinct employee_code) from employee where c.company_code = company_code)\nfrom company c\norder by c.company_code\n```\n# Occupation\n방법1: \n```sql\nselect min(doctor), min(professor),min(singer), min(actor)\nfrom(select \ncase when occupation = 'doctor' then name else null end Doctor,\ncase when occupation = 'professor' then name else null end Professor,\ncase when occupation = 'singer' then name else null end Singer,\ncase when occupation = 'actor' then name else null end Actor,\nrow_number() over (partition by occupation order by name) rnk\nfrom occupations \n) t\ngroup by t.rnk\norder by t.rnk\n```\n방법2: \n```sql\nselect \nmin(case when occupation = \"doctor\" then name else null end) Doctor,\nmin(case when occupation = \"professor\" then name else null end) Professor,\nmin(case when occupation = \"singer\" then name else null end) Singer,\nmin(case when occupation = \"actor\" then name else null end) Actor\n\nfrom(\nselect name,\noccupation,\nrow_number() over (partition by occupation order by name) rnk\nfrom occupations\n) t\ngroup by t.rnk\norder by t.rnk\n```\n방법3: window function 사용하지 않고 풀기. \n```sql\nSET @r1=0, @r2=0, @r3=0. @r4=0;\nselect min(Doctor), min(Professor), min(Singer), min(Actor)\nfrom(\n    select case occupation when 'Doctor' then @r1:=@r1+1\n                           when 'Professor' then @r2:=@r2+1\n                           when 'Singer' then @r3:=@r3+1\n                           when 'Actor' then @r4:=r4+1 END AS RowLine,\n            case when occupation = 'doctor' then name else null end as Doctor,\n            case when occupation = 'professor' then name else null end as Professor,\n            case when occupation = 'singer' then name else null end as Singer,\n            case when occupation = 'actor' then name else null end as Actor\n            from occupation order by name\n)as t\ngroup by RowLine\n```\n\n# Binary Tree Nodes\n방법 1: \n```sql\n-- write a query to find node type of binary tree\n\n-- order by the value of node --> interpreted as order it by n\n\n-- 1. P에 없는 N은 Leaf\n-- 2. P에 있는 N은 inner \n-- 3. P에 null이 있는 같은 열의 N은 Root\n\nselect N,\ncase \nwhen P IS NULL then 'Root'\nwhen N  IN (select distinct P from BST) then 'Inner'\nelse 'Leaf' END \nfrom BST\norder by N\n```\n\n방법 2: join문 활용하기\n```sql\nselect distinct BST.N,\ncase\nwhen BST.P IS NULL then 'Root'\nwhen BST2.P IS NULL then 'Leaf'\nelse 'Inner'\nend\nfrom BST left join BST BST2 on BST.N = BST2.P\norder by n\n```\n\n# SQL Project Planning\n```sql\n-- 1. the start and end dates of projects listed by the number of days it took to complete the project in ascending order. \n-- 2. if more than one project have the same number of completion days\n-- then order by the start date of the project. \nselect t.Start_Date, t2.End_Date\nfrom\n(select Start_Date\n, row_number() over (order by Start_Date) rnk\nfrom Projects\nwhere Start_Date NOT IN (select distinct End_Date from Projects)) t\ninner join (\nselect End_Date\n, row_number() over (order by End_Date) rnk\nfrom projects\nwhere End_Date NOT IN (select distinct Start_Date from Projects)) t2\non t.rnk = t2.rnk\norder by DATEDIFF(end_date,start_date)\n```\n# Ollivander's Inventory\n```sql\n-- 1. write a query to print the id, age, coins_needed, and power of the wands that Ron's interested in, sorted in order of descending power. \n-- 2. If more than one wand has same power, sort the result in order of descending age. \n-- 3. age하고 power가 같은 것중에 coins_needed가 가장 적은 것을 추출해라. \nselect t.id, t.age, t.coins_needed, t.power\nfrom(\nselect\nwands.id as id, \nwands_property.age as age, \nwands.coins_needed as coins_needed, \nwands.power as power,\nrow_number() over (partition by wands_property.age, wands.power order by wands.coins_needed) as rnk, \nwands_property.is_evil as is_evil\nfrom wands inner join wands_property on wands.code = wands_property.code \n) t\nwhere rnk = 1 and t.is_evil = 0\norder by t.power desc, t.age desc\n```\n# Weather Observation 20\n```sql\n-- median \n-- 홀수 n+1/2 번째 숫자가 \n-- 짝수 AVG(n/2,(n/2)+1) 숫자가 median \n-- n은 총 갯수. \n\nselect ROUND(AVG(LAT_N),4)\nfrom \n(\nselect \n    row_number() over(order by LAT_N) rnk,\n    count(*) over () n,\n    LAT_N\nfrom station \n) t\nwhere case \n        when MOD(n,2) =1 then rnk = (n+1)/2\n        ELSE rnk IN (n/2,(n/2)+1) \n      END\n```\n# LeetCode: 262. Trips and Users\n```sql\n\n# 1.write a SQL query to find the cancellation rate of requests with unbanned users (both client and driver must not be banned)\n# 2. each day between \"2013-10-01\" and \"2013-10-03\"\n# 3. cancellation rate =  dividing the umber of canceled/the total number of requests \n# 4. return the result table in any order \n# 5. cancellation rate to two decimal points \n\n# case when Status != 'completed' then 1 else 0 end as total_number_of_cancel,\n# count(*) over () as total_number_of_status\n\nSELECT \nRequest_at as Day, \nROUND(cancelation_rate/total_status_number,2) as 'Cancellation Rate'\n\nFROM(\nSELECT t.Request_at,\nSUM(case when Status != 'completed' then 1 else 0 end) as cancelation_rate,\ncount(*) as total_status_number\nFROM Trips t \n    INNER JOIN Users uc ON t.Client_Id = uc.Users_Id\n    INNER JOIN Users ud ON t.Driver_Id = ud.Users_Id\nWHERE ud.Banned = 'No' AND uc.Banned = 'No' AND t.Request_at BETWEEN '2013-10-01' AND '2013-10-03'\nGROUP BY t.Request_at\n) t2\n```\n# LeetCode: 626. Exchange Seats\n```sql\nSELECT \n    CASE\n        WHEN MOD(id,2) = 1 AND id != total_number then id + 1\n        WHEN MOD(id,2) = 1 AND id = total_number then id \n        ELSE id -1\n        END as id \n    , student\nFROM(\nSELECT *,\nCOUNT(*) OVER () as total_number \nFROM seat\n) t\norder by id \n```","excerpt":"Interviews Contest Leaderboard You did such a great job helping Julia with her last coding contest challenge that she wants you to work on …","fields":{"slug":"/Hacker_Rank_SQL/"},"frontmatter":{"date":"Apr 07, 2021","title":"난이도 중/상 SQL 해커랭크, 리트코드 문제풀이 (총10문제)","tags":["SQL해커랭크_Contest_Leaderboard","SQL해커랭크_Interviews","SQL해커랭크_New Companies","SQL해커랭크_Occupation","SQL해커랭크_Binary Tree Nodes","SQL해커랭크_SQL Project Planning","SQL해커랭크_Ollivander's Inventory","SQL해커랭크_Weather Observation 20","SQL_LeetCode_262. Trips and Users","SQL_LeetCode_626. Exchange Seats","Window_Function","Subquery","With"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"- 프로젝트: 서울시 생활 인구 변화 분석\n- 분석목표: 서울시 생활 인구 2019년 2020년 데이터 EDA 분석, 시각화 \n- 분석환경:\n1. dplyr ==  1.0.4\n2. ggplot2 == 3.3.3\n3. extrafont ==  0.17\n- 담당업무: EDA, 프로젝트 기획, 시각화\n## 1. 전처리\n```R\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata_2019 = read.csv('data/SEOUL_PEOPLE_GU_2019.csv', fileEncoding = 'UTF-8', colClasses=c('시간대'='character'))\ndata_2020 = read.csv('data/SEOUL_PEOPLE_GU_2020.csv', fileEncoding = 'UTF-8', colClasses=c('시간대'='character'))\ndata_2019 %>% head()\ndata_2020 %>% head()\n\n#1. 전처리\n## 2개로 나눠진 데이터를 하나의 데이터로 합치자. \nSP_GU = rbind(data_2019, data_2020)\nSP_GU %>% head(10)\nSP_GU %>% tail(10)\n\nSP_GU %>% str() #기준일의 character가 시간으로 되있는것을 알 수 있다. \n\nSP_GU = SP_GU %>% \n  mutate(기준일 = as.Date(기준일))\nSP_GU %>% str()\n\nSP_GU = SP_GU %>% mutate(연도 = format(기준일,'%Y'))\nSP_GU = SP_GU %>% mutate(요일 = format(기준일, '%u_%a'))\nSP_GU %>% head()\n```\n출력 결과\n![png](./year_day_added.png)\n## 2. 요약 및 시각화\n매년 가장 핫한 holiday는 당연 크리스마스다. 당연 2019의 크리스마스 날 서울 인구 활동은 2020의 크리스마스 서울 인구 활동보다 많을 것이라고 예상하고 코드를 짜보자.\n```R\nSP_GU %>% \n  filter(기준일 %in% c(as.Date('2019-12-24'), as.Date('2020-12-24'))) %>% \n  group_by(기준일) %>% \n  summarise(TOTAL = sum(생활인구수))\n```\n![png](./2019_2020_christmas_populate_rate.png)\n예상했던 데로 2019의 서울이 더 활발하게 움직이는 것을 볼 수 있다. 허나 결과가 조금 이상하다... 우리나라가 중국도 아니고 인구가 2억 명이 넘는다.... 곰곰이 생각해 보니 한 사람이 1시에도 2시에도 3시에도 서울에서 생활을 할 수 있으므로, 중복되어 큰 숫자가 나온 것이라 유추할 수 있다. 중요한 건 2019년과 2020년의 서울 생활 양을 비교하는 것이기 때문에 상대적인 비교만 하면 될 뿐 숫자 자체에 의미를 두지 않았다.\n#### 2-1. 저녁시간대의 크리스마스 이동 인구를 상대적으로 계산해 보자. \n```R\nSP_GU %>% \n  filter(기준일 %in% c(as.Date('2019-12-24'), as.Date('2020-12-24'))) %>% \n  filter(시간대 %in% c('18','19','20','21')) %>% \n  group_by(기준일) %>% \n  summarise(TOTAL = sum(생활인구수))\n```\n![png](./2019_2020_christmas_population_night_rate.png)\n드라마틱한 변화를 감지하진 못했다... 저녁시간의 활동 또한 2019년도가 2020에 앞섰다. \n\n## 3. 자치구별 생활인구수 합계 계산 / 막대그래프 시각화 \n분석의 목표는 \"2019년 대비 2020년의 생활인구 변화의 특성 파악\"이지만 번외로 강남이 땅값이 왜 비싼 지가 궁금해졌다. 내가 알기로는 유동인구가 많아야 땅값이 오른다는데 통상적으로 강남이 항상 땅값이 높으니 당연히 강남 유동인구가 가장 많지 않을까 해서 유동인구를 분석해봤다.\n```R\nagg1 = SP_GU %>% \n  group_by(자치구) %>% \n  summarise(TOTAL = sum(생활인구수))\nagg1 %>% \n  arrange(desc(TOTAL))\n```\n![png](./2019_2020_summed_per_region_population.png)\n```R\nagg1 %>% \n  ggplot(aes(자치구, TOTAL))+\n  geom_col()\nagg1 %>% \n  ggplot(aes(reorder(자치구,-TOTAL),TOTAL)) + \n  geom_col() +\n  theme(axis.text.x = element_text(angle=90))\n```\n![png](./2019_2020_summed_per_region_population_graph.png)\n```R\nagg2 = SP_GU %>% \n  group_by(연도, 자치구) %>% \n  summarise(TOTAL = sum(생활인구수))\n\nagg2 %>%  ggplot(aes(reorder(자치구,-TOTAL),TOTAL,fill=연도)) + geom_col(position='dodge') +\n  theme(axis.text.x = element_text(angle=90))\n```\n![png](./2020_2019_bar_comparison.png)\n역시... 예상대로 강남구, 송파구, 서초구가 2019~2020 서울 유동인구 1,2,3등을 차지했다.생활인구 결과값이 비정상적으로 크게 나온건 개인별, 시간별이 중복될 뿐만 아니라 2019,2020년도를 합해서 계산했기 떄문이다. \n\n## 4. 2020년의 각 자치구별 생활인구의 연령대 비중 계산해보자. 나이 구간별로 생활한 장소가 다를 수 있지 않을까? \n```R\nagg3 = SP_GU %>% \n  filter(연도 =='2020') %>% \n  group_by(자치구, 연령대) %>% \n  summarise(TOTAL = sum(생활인구수)) %>% \n  mutate(PROP = TOTAL/sum(TOTAL))\nagg3\n\nagg3 %>% \n  ggplot(aes(연령대, 자치구, fill=PROP))+\n  geom_tile()+ scale_fill_distiller(palette = 'Blues', direction =1)\n  ```\n  ![png](./chapter_4_1.png)\n 비율이기 때문에 행으로 수치값을 모두 더하면 1이 된다. 색이 진하다는 뜻은 그 지역에 해당 나이 때의 행동반경이 활발했다는 뜻이다. 2020년도에는 10대는 양천구에, 20대는 관악구, 30대는 중구, 40대는 중구, 서초구, 강남구 생활 빈도가 높다는 것을 알 수 있다. 중구, 서초구 강남구는 회사가 밀집한 지역이기 때문에 30대와 40대가 당연히 많을 수밖에 없고 50대 이후부터는 색이 희미해지는 걸 보아 점점 활동량이 감소한다는 것을 유추할 수 있다.\n\n ## 5.연도/자치구/요일별 일평균 생활인구수 열지도 시각화\n ```R\n agg4 = SP_GU %>% \n  group_by(연도, 자치구, 요일, 기준일) %>% \n  summarise(TOTAL = sum(생활인구수))\nagg4\n agg5 = agg4 %>% \n  summarise(MEAN = mean(TOTAL))\nagg5\n\nagg5 %>% \n  ggplot(aes(요일, 자치구, fill=MEAN)) +\n  geom_tile()+\n  facet_wrap(vars(연도))+\n  scale_fill_distiller(palette = 'YlGnBu', direction =1)\n ```\n  ![png](./chapter_5_1.png)\n  2019년도와 2020년도 두 해 모두 월요일부터 일요일까지 송파구와 강남구는 북적거린다. 하지만 강남구의 2019년의 색갈이 2020년의 색깔보다 더 진하다. \n\n  ## 6. tidyr의 spread()를 활용한 형태 변환 및 변화율 계산 \n ```R\nlibrary(tidyr)\nagg5\nagg5 %>% \n  spread(연도, MEAN)\n\nagg6 = agg5 %>% \n  spread(연도, MEAN) %>% \n  mutate(RATIO = `2020`/`2019`)\nagg6\n\nagg6 %>% \n  ggplot(aes(요일, 자치구, fill=RATIO)) + \n  geom_tile() + \n  scale_fill_distiller(palette = 'Reds')\n   ```\n  ![png](./chapter_6_1.png)\n\n  ## 7. 연도/ 연령대/ 요일별 일평균 생활인구수를 계산하고 2019년 대비 2020년의 변화율을 열지도로 시각화 \n```R\n  agg7 = SP_GU %>% \n  group_by(연도, 연령대, 요일, 기준일) %>% \n  summarise(TOTAL = sum(생활인구수))\nagg8 = agg7 %>% \n  summarise(MEAN = mean(TOTAL))\nagg8\nagg9 = agg8 %>% \n  spread(연도, MEAN) %>% \n  mutate(RATIO = `2020`/`2019`)\nagg9\nagg9 %>% \n  ggplot(aes(요일, 연령대,fill=RATIO)) + \n  geom_tile()+\n  scale_fill_distiller(palette = \"Reds\")\n```\n![png](./chapter_7_1.png)\n2019년에 비해 2020년에 생활 반경이 줄어든 나이대는 20대, 60대, 70대인걸 볼 수 있다. 코로나가 터져도 30대, 40대, 50대는 직장을 다녀야 하기 때문에 다른 나이때보다 생활 반경에 영향을 덜 받지 않았나 유추해볼 수 있을것같다. \n또한 전반적으로 서울 전 연령대에 붉은색이 띄는 것을 보아 1이상이 넘는 즉 2019년보다 2020년에 서울에서의 인구 활동이 줄어들었다는 것을 볼 수 있고 그 뜻은 경기권이나 타 지역에서 서울로 유입되는 사람들이 줄어들었다라고 해석할 수도 있겠다.\n\n7가지 방법으로 2019년도와 2020년도의 서울시 인구생활량을 분석해봤고 역시 예상했던 데로 2019년이 2020년도보다 활동량이 많았다.\n지역적으로 본다면 강남구, 송파구, 서초구가 가장 활동량이 많았으며 나이대를 본다면 30대~40대의 활동량이 가장 많았다. 즉 종합해 본다면 일을 하기 위한 활동량을 코로나로 많은 영향을 받지 않은 것을 분석을 통해 알 수 있었다.","excerpt":"프로젝트: 서울시 생활 인구 변화 분석 분석목표: 서울시 생활 인구 2019년 2020년 데이터 EDA 분석, 시각화  분석환경: dplyr ==  1.0.4 ggplot2 == 3.3.3 extrafont ==  0.17 담당업무: EDA, 프로젝…","fields":{"slug":"/R_change_of_population/"},"frontmatter":{"date":"Mar 11, 2021","title":"R로 서울시 생활인구 변화 분석 (탐색적 분석)","tags":["데이터 분석 프로젝트","데이터 분석 입사 프로젝트","R studio 탐색적 분석하기","서울생활인구 분석"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\nWorkbench, sequel pro를 aws로 연결해서 sql을 연습하다 oracle을 연습해보고 싶어졌다. 허나 oracle에 가보니 오라클 18c Express Edition이 윈도우만 지원하는것 보고 하늘이시여.. 또 몇시간 블로그 보며 파해쳐야 겠구나 했다. \n\n오늘은 Mac_Lover들이 oracle을 docker로 다운로드 하는 방법을 포스팅 해보려고 한다. \n\n우선 크게 보면 \n\n**1. Docker 설치**\n\n**2. Terminal 에서 oracle 11g 설치**\n\n**3. SQL Plus 실행하기**\n\n로 보면 되겠다.\n\n**1. Docker 설치**\n- https://www.docker.com/products/docker-desktop\n1-1. 도커를 다운로드 하면 앱에서 창에 복붙해야하는 명령어가 뜬다. 그 명령어를 터미널 창에 복붙하자.\n![png](./step1.png)\n1-2. 복붙이 끝나고 실행이 되면 우측 상단과 중간 사이에 귀여운 고래 모양이 뜬다. 고래 모양이 뜨면 잘 실행되고 있다는 것.\n![png](./step2.png)\n**2. Terminal 에서 oracle 11g 설치하기.**\n2-1. 터미널에서 **docker search oracle-xe-11g** 명령어를 입력해서 다운로드 할 이미지를 검색한다.검색한 이미지 목록을 밑으로 쭉 내리다 보면 jaspeen/oracle-xe-11g가 보일텐데 이 것을 사용해서 이미지를 다운로드 해보자.\n![png](./step3.png)\n2-2. docker pull jaspeen/oracle-xe-11g 명령어를 입력하면 아래와 같은 화면이 나타난다.\n![png](./step4.png)\n2-3. docker images 명령어를 사용하여 jaspeen/oracle-xe-11g 이미지가 다운로드 되었는지 확인한다.\n![png](./step5.png)\n2-4. 이제 컨테이너 생성을 해 볼 차례이다.\n**docker run --name oracle -d -p 8080:8080 -p 1521:1521 jaspeen/oracle-xe-11g** 명령어를 입력하여 컨테이너의 생성과 실행을 해준다.\n여기서 **oracle 은 임의로 설정한 컨테이너의 이름**으로, 본인이 원하는 이름을 넣어 명령문을 작성하면 된다.내가 입력한 oracle 의 컨테이너가 생성되고\n**docker ps**를 입력해서 컨테이너가 제대로 실행되는지 (NAMES에 지정한 이름이 뜨는지) 확인한다.\n![png](./step6.png)\n2-5. **docker exec -it oracle sqlplus** 명령문으로 sqlplus를 시작할 수 있다.여기서 oracle 은 컨테이너를 생성할때 만든 임의의 컨테이너 이름으로, 각자가 정한 이름을 넣어주면 된다.\n2-6. Enter user-name : 에는 **system** 을 입력하고 Enter password: 에는 본인이 원하는 비번을 입력하면 되는데 비밀번호는 입력해도 화면상으로 보이지 않기 때문에 오타에 유의한다.둘 다 정확히 입력했다면 Connecte to: 와 함께 SQL> 이 뜬다\n![png](./step7.png)\n**3. SQLPlus가 실행된다.**","excerpt":"Workbench, sequel pro를 aws로 연결해서 sql을 연습하다 oracle을 연습해보고 싶어졌다. 허나 oracle에 가보니 오라클 18c Express Edition이 윈도우만 지원하는것 보고 하늘이시여.. 또 몇시간 블로그 보며 파…","fields":{"slug":"/Oracle_Docker_Download/"},"frontmatter":{"date":"Mar 02, 2021","title":"Docker로 Oracle다운받기","tags":["Dockerdownload","mac도커다운로드","mac오라클다운로드하는방법","맥오라클다운로드"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n함수 f(x,y) = $2x^2 + 6xy + 7y^2 -26x -54y + 107$로 표현되는 지형을 그래프로 나타내고, 지형의 (14,4)지점에 공을 두었다면 어떤 경로로 공이 움직일지 경로를 그려라\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sympy\nimport matplotlib as mpl\nimport scipy as sp\nimport pandas as pd\n```\n\n\n```python\nblack = {'facecolor':'black'}\ndef f(x,y):\n    return 2*x**2+6*x*y+7*y**2-26*x-54*y+107\nxx = np.linspace(1,16,100)\nyy = np.linspace(-3,6,90)\nX, Y = np.meshgrid(xx,yy)\nZ=f(X,Y)\n\ndef gx(x,y):\n    return 4*x+6*y-26\ndef gy(x,y):\n    return 6*x+14*y-54\n\nxx2 = np.linspace(1,16,15)\nyy2 = np.linspace(-3,6,9)\nX2,Y2 = np.meshgrid(xx2,yy2)\nGX = gx(X2,Y2)\nGY = gy(X2,Y2)\nplt.figure(figsize=(10,5))\nplt.contour(X,Y,Z, levels=np.logspace(0,3,10))\n\nx0=(14,4)\nplt.plot(x0[0],x0[1],'ko',ms=10)\n\nfor i in range(20):\n    g=np.array((gx(x0[0],x0[1]),gy(x0[0],x0[1])))\n    x_next= x0-0.02*g\n    plt.annotate('',xy=x_next,xytext=x0,arrowprops=black)\n    x0= x_next\n    plt.quiver(X2,Y2,GX,GY,color='blue', scale=400, minshaft=2)\n\nplt.xlabel('x')\nplt.ylabel('y')\n\nplt.show()\n```\n\n\n![png](./chapter4_from_page_317_to_318_2_0.png)\n\n\n\n","excerpt":"함수 f(x,y) = 로 표현되는 지형을 그래프로 나타내고, 지형의 (14,4)지점에 공을 두었다면 어떤 경로로 공이 움직일지 경로를 그려라","fields":{"slug":"/chapter4_from_page_317_to_318_files/"},"frontmatter":{"date":"Jan 22, 2021","title":"기울기 벡터를 이용한 응용 문제 풀이.","tags":["Matplotlib","gredient vector","기울기 벡터","퀴버플로"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"### Steps to get a gradient vector.      \n- given equation: f(x,y) = $2x^2 + 6xy + 7y^2 -26x -54y + 107$\n- first, cal with respect to x and y each. Then you will get a two derivative equations\n\n\\begin{equation}\n    \\nabla (f)= \\begin{bmatrix}\n4x+6y-26\\\\\n6x+14y-54\n\\end{bmatrix}\n\\end{equation}\n\n- second, choose the point where you want to get a slope each. Then apply the point.\n    - x=7,y=1\n        - then you will get an (8,2) which is a slope of each derevative. \n- at last, try to get a gradient vector by calculating the following : $\\sqrt{8^2+2^2}$\n\n\n\n\n```python\nimport numpy as np\nimport matplotlib.pylab as plt\n\nblack = {'facecolor':'black'}\ndef g(x,y):\n    return (4*x+6*y-26, 6*x+14*y-54)\ng1 = g(7,1)\ng2 = g(2,1)\n\nplt.plot(0,0,'kP',ms=10)\nplt.plot(g1[0],g1[1],'ro',ms=10)\nplt.annotate('',xy=g1,xytext=(0,0),arrowprops=black)\nplt.plot(g2[0],g2[1],'ro',ms=10)\nplt.annotate('',xy=g2,xytext=(0,0),arrowprops=black)\n\nplt.axis('equal')\nplt.show()\n```\n\n\n![png](./Cal_Matrix_chapter4_from_page_313_to_316_1_0.png)\n\n\nFollowing graph is to show the function : f(x,y) = $2x^2 + 6xy + 7y^2 -26x -54y + 107$\n\n\n```python\ndef f(x,y):\n    return 2*x**2+6*x*y+7*y**2-26*x-54*y+107\nxx = np.linspace(1,16,100)\nyy = np.linspace(-3,6,90)\nX,Y = np.meshgrid(xx,yy)\nZ=f(X,Y)\n\ndef gx(x,y):\n    return 4*x+6*y-26\ndef gy(x,y):\n    return 6*x+14*y-54\n\nxx2 = np.linspace(1,16,15)\nyy2 = np.linspace(-3,6,9)\n\nX2, Y2 = np.meshgrid(xx2,yy2)\nGX = gx(X2,Y2)\nGY = gy(X2,Y2)\nplt.figure(figsize = (10,5))\nplt.contour(X,Y,Z, levels=np.logspace(0,3,10))\nplt.quiver(X2, Y2, GX, GY, color='blue', scale=400, minshaft=2)\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('quiver plot')\nplt.show()\n```\n\n\n![png](./Cal_Matrix_chapter4_from_page_313_to_316_3_0.png)\n\n\n\n```python\n\n```\n","excerpt":"Steps to get a gradient vector. given equation: f(x,y) =  first, cal with respect to x and y each. Then you will get a two derivative equat…","fields":{"slug":"/Cal_Matrix_chapter4_from_page_313_to_316_files/"},"frontmatter":{"date":"Jan 18, 2021","title":"Matplotlib을 이용해 기울기 벡터 퀴버플롯 그래프 표현.","tags":["Matplotlib","gredient vector","기울기 벡터","퀴버플로"],"update":"Jan 01, 0001"}}}]}},"pageContext":{}},"staticQueryHashes":["2027115977","694178885"]}